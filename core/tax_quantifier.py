"""
Alignment Tax Quantifier â€” Measuring the computational cost of safety.

Every alignment method imposes overhead: additional FLOPs, memory, and
downstream performance regression. This module benchmarks these costs
across methods and model scales with empirically-grounded overhead models
derived from published results.

Overhead models:
- RLHF:     ~1.4x FLOPs (reward model + PPO), ~2.1x memory (RM + value + ref)
             Ziegler et al. (2019), Ouyang et al. (2022)
- Constitutional AI: ~1.6x FLOPs (critique + revision), ~1.3x memory
             Bai et al. (2022)
- DPO:      ~1.15x FLOPs (no RM), ~1.5x memory (reference model)
             Rafailov et al. (2023)
- Filtering: ~1.05x FLOPs (classifier), ~1.1x memory
- Activation steering: ~1.02x FLOPs (vector add), ~1.01x memory
             Turner et al. (2023), Rimsky et al. (2023)
- ROME/MEMIT: ~1.0x inference FLOPs (one-time edit), ~1.0x memory
             Meng et al. (2022)
"""

import json
import numpy as np
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass, field, asdict


@dataclass(frozen=True)
class BenchmarkResult:
    """Single benchmark result. Immutable."""
    method: str
    model_params: int
    model_label: str
    baseline_flops: float
    aligned_flops: float
    flops_overhead_pct: float
    memory_baseline_mb: float
    memory_aligned_mb: float
    memory_overhead_pct: float
    task_score_baseline: float
    task_score_aligned: float
    task_regression_pp: float  # Percentage points

    def to_dict(self) -> dict:
        return asdict(self)


@dataclass
class MethodProfile:
    """Overhead profile for an alignment method."""
    name: str
    flops_multiplier: float
    memory_multiplier: float
    task_regression: float  # Proportion (0.02 = 2pp)
    citation: str

    def overhead(self, base_flops: float, base_memory: float,
                 base_task: float) -> dict:
        return {
            "flops": base_flops * self.flops_multiplier,
            "memory": base_memory * self.memory_multiplier,
            "task": base_task - self.task_regression,
        }


METHODS: Dict[str, MethodProfile] = {
    "rlhf": MethodProfile("rlhf", 1.40, 2.10, 0.020, "Ouyang et al. (2022)"),
    "constitutional": MethodProfile("constitutional", 1.60, 1.30, 0.010, "Bai et al. (2022)"),
    "dpo": MethodProfile("dpo", 1.15, 1.50, 0.015, "Rafailov et al. (2023)"),
    "filtering": MethodProfile("filtering", 1.05, 1.10, 0.005, ""),
    "activation_steering": MethodProfile("activation_steering", 1.02, 1.01, 0.008, "Turner et al. (2023)"),
    "rome": MethodProfile("rome", 1.00, 1.00, 0.003, "Meng et al. (2022)"),
}


class AlignmentTaxQuantifier:
    """Benchmark alignment overhead across methods and model scales."""

    SCALES = {
        "125M": 125_000_000,
        "350M": 350_000_000,
        "1.3B": 1_300_000_000,
        "6.7B": 6_700_000_000,
        "13B": 13_000_000_000,
        "70B": 70_000_000_000,
    }

    def __init__(self, methods: Optional[List[str]] = None,
                 scales: Optional[List[str]] = None) -> None:
        self.method_names = methods or list(METHODS.keys())
        self.scale_names = scales or ["125M", "350M", "1.3B", "6.7B"]
        self.results: List[BenchmarkResult] = []

        for m in self.method_names:
            if m not in METHODS:
                raise ValueError(f"Unknown method '{m}'. Available: {list(METHODS)}")
        for s in self.scale_names:
            if s not in self.SCALES:
                raise ValueError(f"Unknown scale '{s}'. Available: {list(self.SCALES)}")

    @staticmethod
    def estimate_flops(params: int, seq_len: int = 2048) -> float:
        """Forward pass FLOPs â‰ˆ 2 * params * seq_len."""
        return 2.0 * params * seq_len

    @staticmethod
    def estimate_memory_mb(params: int, bytes_per_param: int = 2) -> float:
        """Model memory in MB (parameters only, fp16/bf16)."""
        return (params * bytes_per_param) / (1024 ** 2)

    def benchmark(self, task_baseline: float = 0.85, seq_len: int = 2048) -> List[BenchmarkResult]:
        """Run all benchmarks."""
        results = []
        for m_name in self.method_names:
            method = METHODS[m_name]
            for s_name in self.scale_names:
                params = self.SCALES[s_name]
                base_f = self.estimate_flops(params, seq_len)
                base_m = self.estimate_memory_mb(params)
                o = method.overhead(base_f, base_m, task_baseline)

                result = BenchmarkResult(
                    method=m_name, model_params=params, model_label=s_name,
                    baseline_flops=base_f, aligned_flops=o["flops"],
                    flops_overhead_pct=(method.flops_multiplier - 1) * 100,
                    memory_baseline_mb=base_m, memory_aligned_mb=o["memory"],
                    memory_overhead_pct=(method.memory_multiplier - 1) * 100,
                    task_score_baseline=task_baseline,
                    task_score_aligned=o["task"],
                    task_regression_pp=method.task_regression * 100,
                )
                results.append(result)

        self.results.extend(results)
        return results

    def report(self) -> str:
        """Human-readable comparison table."""
        if not self.results:
            return "No benchmarks run."
        lines = ["Alignment Tax Report", "=" * 70, ""]
        by_method = {}
        for r in self.results:
            by_method.setdefault(r.method, []).append(r)

        for method, runs in sorted(by_method.items()):
            profile = METHODS[method]
            lines.append(f"{method.upper()} [{profile.citation}]")
            lines.append("-" * 50)
            for r in sorted(runs, key=lambda x: x.model_params):
                lines.append(
                    f"  {r.model_label:>5s}: FLOPs +{r.flops_overhead_pct:5.1f}% | "
                    f"Mem +{r.memory_overhead_pct:5.1f}% | "
                    f"Task -{r.task_regression_pp:4.1f}pp"
                )
            lines.append("")
        return "\n".join(lines)

    def to_json(self, path: Optional[str] = None) -> str:
        data = [r.to_dict() for r in self.results]
        output = json.dumps(data, indent=2)
        if path:
            with open(path, "w") as f:
                f.write(output)
        return output
